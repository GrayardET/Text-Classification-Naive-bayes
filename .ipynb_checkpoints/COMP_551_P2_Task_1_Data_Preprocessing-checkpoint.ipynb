{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me4bLMo3nG6C"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCagW4Umm6jY"
   },
   "source": [
    "### Implement the package needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ddz5UA_rZZpH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKP1qvtrnQtB"
   },
   "source": [
    "##Processing dataset *twenty news groups*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVBERDYp91da"
   },
   "source": [
    "### Simplest data processing with no resctriction (Version 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqYxdg5Xng3w",
    "outputId": "ffb26cb3-4a1d-461c-bb95-8ec9baaa77e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of labels:11314\n",
      "shape of vectorizer:(11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "twenty_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "twenty_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "\n",
    "#twenty_train.target_names\n",
    "#print(len(twenty_train.data))\n",
    "#print(len(twenty_train.filenames))\n",
    "\n",
    "#Initialize the model used to transfrom the data from text to vector\n",
    "#with special condition to increase the accuracy.\n",
    "vectorizer_v1 = CountVectorizer()\n",
    "labels_train_twenty = twenty_train.target\n",
    "labels_test_twenty = twenty_test.target\n",
    "\n",
    "\n",
    "twenty_train_v1 = vectorizer_v1.fit_transform(twenty_train.data)\n",
    "twenty_test_v1 = vectorizer_v1.transform(twenty_test.data)\n",
    "\n",
    "\n",
    "print(\"shape of labels:\" + str(len(labels_train)))\n",
    "print(\"shape of vectorizer:\" + str(twenty_train_v1.toarray().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwxZDrn--Pew"
   },
   "source": [
    "### Take care of only stop_words and meaningless combination of number and letters (Version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiRQ9AaP-hUp",
    "outputId": "5b1dae62-074d-47f9-8b84-a67b4538d2dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pjy_t\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 72790)\n",
      "(11314,)\n",
      "(7532, 72790)\n"
     ]
    }
   ],
   "source": [
    "#Take care of the stop_words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "\n",
    "# print(sw)\n",
    "\n",
    "vectorizer_v2 = CountVectorizer(stop_words = sw, token_pattern = '[a-z]+')\n",
    "\n",
    "twenty_train_v2 = vectorizer_v2.fit_transform(twenty_train.data)\n",
    "twenty_test_v2 = vectorizer_v2.transform(twenty_test.data)\n",
    "#print(vectorizer_v2.get_feature_names())\n",
    "#print(type(twenty_test_v2.toarray()))\n",
    "print(twenty_train_v2.shape)\n",
    "print(labels_train.shape)\n",
    "print(twenty_test_v2.shape)\n",
    "\n",
    "\n",
    "# print(np.array(twenty_train_v2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQhdo2vK-tgJ"
   },
   "source": [
    "### Take care of high frequency words and low frequency words (Version 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnSHvIN1Bnfv",
    "outputId": "8aea8335-5d69-4b9e-a70d-21600049ae1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 9349)\n",
      "(11314,)\n",
      "(7532, 9349)\n"
     ]
    }
   ],
   "source": [
    "vectorizer_v3 = CountVectorizer(max_df = 0.5)\n",
    "\n",
    "twenty_train_v3 = vectorizer_v3.fit_transform(twenty_train.data)\n",
    "twenty_test_v3 = vectorizer_v3.transform(twenty_test.data)\n",
    "\n",
    "#print(twenty_train_v3.shape)\n",
    "# print(vectorizer_v3.get_feature_names())\n",
    "\n",
    "vectorizer_v4 = CountVectorizer(min_df = 0.001)\n",
    "\n",
    "twenty_train_v4 = vectorizer_v4.fit_transform(twenty_train.data)\n",
    "twenty_test_v4 = vectorizer_v4.transform(twenty_test.data)\n",
    "print(twenty_train_v4.shape)\n",
    "print(labels_train.shape)\n",
    "print(twenty_test_v4.shape)\n",
    "\n",
    "\n",
    "#print(vectorizer_v4.get_feature_names())sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Rs5Zvv4bMEL"
   },
   "source": [
    "### Tfidf data processing on different version of countVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AOtDoMGDbtu5"
   },
   "outputs": [],
   "source": [
    "#Using a new model to eliminate the problem of different weight on words. Using machine instead of manually\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "twenty_train_tfidf_v1 = tfidf_transformer.fit_transform(twenty_train_v1)\n",
    "twenty_test_tfidf_v1 = tfidf_transformer.transform(twenty_test_v1)\n",
    "#print(twenty_train_tfidf.shape)\n",
    "\n",
    "twenty_train_tfidf_v2 = tfidf_transformer.fit_transform(twenty_train_v2)\n",
    "twenty_test_tfidf_v2 = tfidf_transformer.transform(twenty_test_v2)\n",
    "\n",
    "twenty_train_tfidf_v3 = tfidf_transformer.fit_transform(twenty_train_v3)\n",
    "twenty_test_tfidf_v3 = tfidf_transformer.transform(twenty_train_v3)\n",
    "\n",
    "twenty_train_tfidf_v4 = tfidf_transformer.fit_transform(twenty_train_v4)\n",
    "twenty_test_tfidf_v4 = tfidf_transformer.transform(twenty_train_v4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg0x0bAKnnf2"
   },
   "source": [
    "## Processing dataset *IMDB review*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_train():\n",
    "    with open('imdb_train.pickle', 'rb') as handle:\n",
    "        result = pickle.load(handle)\n",
    "    return result\n",
    "\n",
    "def load_imdb_test():\n",
    "    with open('imdb_test.pickle', 'rb') as handle:\n",
    "        result = pickle.load(handle)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please download data from http://ai.stanford.edu/~amaas/data/sentiment/ and upload to collab and put under folder \"content\"\n",
    "\n",
    "#Uncompress the aclimdb)v1.tar.gz\n",
    "import os\n",
    "# os.system('tar zxvf %s' % 'aclImdb_v1.tar.gz' )\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "#Import data into the code\n",
    "# from sklearn.datasets import load_files\n",
    "# imdb_train = load_files('aclImdb/train')\n",
    "# imdb_test = load_files('aclImdb/test')\n",
    "#-----------------------------------------------------------------------\n",
    "# with open('imdb_train.pickle', 'wb') as handle:\n",
    "#     pickle.dump(imdb_train ,handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('imdb_test.pickle', 'wb') as handle:\n",
    "#     pickle.dump(imdb_test, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "# OS takes a long time to load files from disk, therefore we saved the imdb_train and imdb_test set using pickle\n",
    "imdb_train = load_imdb_train()\n",
    "imdb_test = load_imdb_test()\n",
    "labels_train_imdb = imdb_train.target\n",
    "labels_test_imdb = imdb_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7y-BcS9Wk7kc"
   },
   "source": [
    "### Simplest data processing with no restriction (Version 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "5R-dlRWynrT2",
    "outputId": "939882a2-563a-492f-e9db-f056052432a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 2987)\t1\n",
      "  (0, 3598)\t1\n",
      "  (0, 5618)\t3\n",
      "  (0, 7054)\t2\n",
      "  (0, 7611)\t2\n",
      "  (0, 11008)\t1\n",
      "  (0, 11043)\t2\n",
      "  (0, 18502)\t1\n",
      "  (0, 19732)\t1\n",
      "  (0, 22001)\t1\n",
      "  (0, 23825)\t1\n",
      "  (0, 30916)\t1\n",
      "  (0, 32350)\t1\n",
      "  (0, 41152)\t1\n",
      "  (0, 43231)\t1\n",
      "  (0, 43297)\t1\n",
      "  (0, 46381)\t1\n",
      "  (0, 46419)\t1\n",
      "  (0, 46735)\t2\n",
      "  (0, 49567)\t2\n",
      "  (0, 50002)\t2\n",
      "  (0, 50538)\t1\n",
      "  (0, 51206)\t1\n",
      "  (0, 53345)\t1\n",
      "  (0, 54396)\t2\n",
      "  :\t:\n",
      "  (24999, 110234)\t1\n",
      "  (24999, 110272)\t1\n",
      "  (24999, 110367)\t1\n",
      "  (24999, 110852)\t1\n",
      "  (24999, 110866)\t1\n",
      "  (24999, 111180)\t5\n",
      "  (24999, 111517)\t3\n",
      "  (24999, 113193)\t1\n",
      "  (24999, 113331)\t1\n",
      "  (24999, 113472)\t1\n",
      "  (24999, 113602)\t1\n",
      "  (24999, 115332)\t1\n",
      "  (24999, 116398)\t1\n",
      "  (24999, 117102)\t1\n",
      "  (24999, 117371)\t1\n",
      "  (24999, 118219)\t1\n",
      "  (24999, 119561)\t3\n",
      "  (24999, 119773)\t1\n",
      "  (24999, 120192)\t1\n",
      "  (24999, 120388)\t1\n",
      "  (24999, 121904)\t1\n",
      "  (24999, 121928)\t1\n",
      "  (24999, 122910)\t1\n",
      "  (24999, 123190)\t3\n",
      "  (24999, 123257)\t1\n"
     ]
    }
   ],
   "source": [
    "#Train the data using the same special vectorize as above\n",
    "imdb_train_v1 = vectorizer_v1.fit_transform(imdb_train.data)\n",
    "imdb_test_v1 = vectorizer_v1.transform(imdb_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ2CjXmBk6e9"
   },
   "source": [
    "### Take care of only stop_words and meaningless combination of number and letters (Version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "VLTyLx1NlNUj"
   },
   "outputs": [],
   "source": [
    "imdb_train_v2 = vectorizer_v2.fit_transform(imdb_train.data)\n",
    "imdb_test_v2 = vectorizer_v2.transform(imdb_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgcysyxwlmuv"
   },
   "source": [
    "### Take care of high frequency words and low frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "R_KrcNbjlslX"
   },
   "outputs": [],
   "source": [
    "imdb_train_v3 = vectorizer_v3.fit_transform(imdb_train.data)\n",
    "imdb_test_v3 = vectorizer_v3.transform(imdb_test.data)\n",
    "\n",
    "imdb_train_v4 = vectorizer_v4.fit_transform(imdb_train.data)\n",
    "imdb_test_v4 = vectorizer_v4.transform(imdb_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_v5 = CountVectorizer(stop_words = sw, token_pattern = '[a-z]+', min_df = 0.001)\n",
    "imdb_train_v5 = vectorizer_v5.fit_transform(imdb_train.data)\n",
    "imdb_test_v5 = vectorizer_v5.transform(imdb_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 10001)\n",
      "(25000, 10001)\n",
      "(75000, 120682)\n",
      "(11314, 72790)\n",
      "(7532, 72790)\n"
     ]
    }
   ],
   "source": [
    "print(imdb_train_v5.shape)\n",
    "print(imdb_test_v5.shape)\n",
    "print(imdb_train_v2.shape)\n",
    "print(twenty_train_v2.shape)\n",
    "print(twenty_test_v2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POaFpx5olxKp"
   },
   "source": [
    "### Tfidf data processing on different version of countVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uehUO7s5odhz"
   },
   "source": [
    "# Task 2: Implement Naive Bayes and k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CKxbmHLo01R"
   },
   "source": [
    "## Fitting the data\n",
    "Below, first we implement the `fit` function that learns the model parameters. We use Laplace smoothing for the class prior using $\\alpha=\\beta=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultinomialNaiveBayes:\n",
    "#     def __init__(self):\n",
    "#         return\n",
    "\n",
    "#     def fit(self, x, y, sigma=1):\n",
    "#         N, D = x.shape                          # get the shape from input data\n",
    "#         C = np.max(y) + 1                       # number of class(y)\n",
    "#         prob_table = np.zeros((C,D))            # table with occurance of word given class c. \n",
    "#                                                 # It goes from y = 1, ... , y = C\n",
    "#         # From here on the implementation will be different, since we're dealing with multinomial dist.\n",
    "#         Nc = np.zeros(C)\n",
    "#         for c in range(C):\n",
    "#             x_c = x[y == c]                     # slice all elements with a given label y\n",
    "#             Nc[c] = x_c.shape[0]                # number of elements of class c\n",
    "#             # calculates probability table with smoothing, default: alpha = beta = 1\n",
    "#             prob_table[c,:] = (sigma + np.sum(x_c, axis = 0))  / (sigma*C  + np.sum(x_c))\n",
    "        \n",
    "#         # Prior believe with smoothing (alpha_y, beta_y)\n",
    "#         self.pi = (Nc + sigma)/(np.sum(Nc) + sigma*C)\n",
    "#         self.prob_table = prob_table\n",
    "#         return self\n",
    "\n",
    "        \n",
    "#     def logsumexp(self, Z):                                                # dimension C x N\n",
    "#         Zmax = np.max(Z,axis=0)                             # max over C\n",
    "#         log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
    "#         return log_sum_exp\n",
    "\n",
    "    \n",
    "#     def predict(self, input):\n",
    "#         # The input is a numpy array of shape (C, D), it's acquired by fitting the test set\n",
    "#         # on the vectorizer for training.\n",
    "\n",
    "#         C, D = self.prob_table.shape            # obtain dimension of probability matrix\n",
    "#         A = input.shape[0]                      # size of test set\n",
    "#         # for numerical stability we work in the log domain\n",
    "#         # we add a dimension because this is added to the log-likelihood matrix \n",
    "#         # that assigns a likelihood for each class (C) to each test point, and so it is C x N\n",
    "#         log_prior = np.log(self.pi)\n",
    "# #         print(\"prior = \" + str(self.pi))\n",
    "# #         print(log_prior.shape)\n",
    "#         result = np.zeros((A, C))   # A x C\n",
    "#         print(result.shape)\n",
    "#         for a in range (A):\n",
    "#             log_prob = np.zeros(C)\n",
    "#             if a % 1000 == 0:\n",
    "#                 print(f\"Currently processing instance {a}\")\n",
    "#             # loop over all class labels \n",
    "#             for c in range(C):\n",
    "#                 # show progress since it takes a long tim\n",
    "#                 temp_c = self.prob_table[c]\n",
    "#                 exist = temp_c[input[a] > 0]\n",
    "#                 notExist = temp_c[input[a] == 0]\n",
    "#                 log_prob[c] = np.sum(np.log(exist))+np.sum(np.log(1-notExist)) \n",
    "#                 log_posterior = log_prior + log_prob\n",
    "# #                 print(\"log_prior.shape: \" + str(log_prior.shape))\n",
    "# #                 print(\"log_prob.shape: \" + str(log_prob.shape))\n",
    "# #                 print(\"dimension of log_posterior: \" + str(log_posterior.shape))\n",
    "#                 # normalization involving logsumexp trick\n",
    "#                 result[a,:] = np.exp(log_posterior - self.logsumexp(log_posterior))\n",
    "#         return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b26CumjZIP03"
   },
   "source": [
    "## Multinomial model for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4MrVBHbnIg4U"
   },
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def fit(self, x, y, sigma=1):\n",
    "        N, D = x.shape                          # get the shape from input data\n",
    "        C = np.max(y) + 1                       # number of class(y)\n",
    "        prob_table = np.zeros((C,D))            # table with occurance of word given class c. \n",
    "                                                # It goes from y = 1, ... , y = C\n",
    "        # From here on the implementation will be different, since we're dealing with multinomial dist.\n",
    "        Nc = np.zeros(C)\n",
    "        for c in range(C):\n",
    "            x_c = x[y == c]                     # slice all elements with a given label y\n",
    "            Nc[c] = x_c.shape[0]                # number of elements of class c\n",
    "            # calculates probability table with smoothing, default: alpha = beta = 1\n",
    "            prob_table[c,:] = (sigma + np.sum(x_c, axis = 0))  / (sigma*C  + np.sum(x_c))\n",
    "        \n",
    "        # Prior believe with smoothing (alpha_y, beta_y)\n",
    "        self.pi = (Nc + sigma)/(np.sum(Nc) + sigma*C)\n",
    "        self.prob_table = prob_table\n",
    "        return self\n",
    "\n",
    "        \n",
    "    def logsumexp(self, Z):                                                # dimension C x N\n",
    "        Zmax = np.max(Z,axis=0)                             # max over C\n",
    "        log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
    "        return log_sum_exp\n",
    "\n",
    "    \n",
    "    def predict(self, input):\n",
    "        # The input is a numpy array of shape (C, D), it's acquired by fitting the test set\n",
    "        # on the vectorizer for training.\n",
    "        # Sigma being the normalization constant.\n",
    "\n",
    "        C, D = self.prob_table.shape            # obtain dimension of probability matrix\n",
    "        A = input.shape[0]                      # size of test set\n",
    "        # for numerical stability we work in the log domain\n",
    "        # we add a dimension because this is added to the log-likelihood matrix \n",
    "        # that assigns a likelihood for each class (C) to each test point, and so it is C x N\n",
    "        log_prior = np.log(self.pi)\n",
    "#         print(\"prior = \" + str(self.pi))\n",
    "#         print(log_prior.shape)\n",
    "        result = np.zeros((A, C))   # A x C\n",
    "        print(result.shape)\n",
    "        for a in range (A):\n",
    "            log_prob = np.zeros(C)\n",
    "            if a % 1000 == 0:\n",
    "                print(f\"Currently processing instance {a}\")\n",
    "            # loop over all class labels \n",
    "            for c in range(C):\n",
    "                # show progress since it takes a long tim\n",
    "                temp_c = self.prob_table[c]\n",
    "                exist = temp_c[input[a] > 0]\n",
    "                notExist = temp_c[input[a] == 0]\n",
    "                log_prob[c] = np.sum(np.log(exist))+np.sum(np.log(1-notExist)) \n",
    "                log_posterior = log_prior + log_prob\n",
    "#                 print(\"log_prior.shape: \" + str(log_prior.shape))\n",
    "#                 print(\"log_prob.shape: \" + str(log_prob.shape))\n",
    "#                 print(\"dimension of log_posterior: \" + str(log_posterior.shape))\n",
    "                # normalization involving logsumexp trick\n",
    "                result[a,:] = np.exp(log_posterior - self.logsumexp(log_posterior))\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM1GmtnVytbB"
   },
   "source": [
    "### Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "G-P5YIkO4eyo"
   },
   "outputs": [],
   "source": [
    "def evaluate_acc(y, y_hat):\n",
    "    accuracy = np.sum(y == y_hat) / y.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWfU96ldPqKl"
   },
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1-_KrG7Pr3u",
    "outputId": "174f6604-34f6-4759-f55e-cee86b31d510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7532, 20)\n",
      "Currently processing instance 0\n",
      "Currently processing instance 1000\n",
      "Currently processing instance 2000\n",
      "Currently processing instance 3000\n",
      "Currently processing instance 4000\n",
      "Currently processing instance 5000\n",
      "Currently processing instance 6000\n",
      "Currently processing instance 7000\n",
      "test accuracy: 0.582979288369623\n"
     ]
    }
   ],
   "source": [
    "model1 = MultinomialNaiveBayes()\n",
    "model1.fit(twenty_train_v4.toarray(), labels_train_twenty, 1)\n",
    "y_prob = model1.predict(twenty_test_v4.toarray())\n",
    "# check_sum =  np.sum(y_prob,axis = 1)\n",
    "# print(y_prob.shape)\n",
    "# print(y_prob)\n",
    "# print(check_sum.T)\n",
    "y_pred = np.argmax(y_prob, axis = 1)\n",
    "# print(y_pred.shape)\n",
    "model1.accuracy = evaluate_acc(y = labels_test, y_hat = y_pred)\n",
    "print(f'test accuracy: {model1.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 9349)\n",
      "(7532, 20)\n",
      "Currently processing instance 0\n",
      "Currently processing instance 1000\n",
      "Currently processing instance 2000\n",
      "Currently processing instance 3000\n",
      "Currently processing instance 4000\n",
      "Currently processing instance 5000\n",
      "Currently processing instance 6000\n",
      "Currently processing instance 7000\n",
      "test accuracy: 0.6018321826872013\n"
     ]
    }
   ],
   "source": [
    "model2 = MultinomialNaiveBayes()\n",
    "model2.fit(twenty_train_v2.toarray(), labels_train_twenty,1)\n",
    "y_prob = model2.predict(twenty_test_v2.toarray())\n",
    "# check_sum =  np.sum(y_prob,axis = 1)\n",
    "# print(y_prob.shape)\n",
    "# print(y_prob)\n",
    "# print(check_sum.T)\n",
    "y_pred = np.argmax(y_prob, axis = 1)\n",
    "# print(y_pred.shape)\n",
    "\n",
    "\n",
    "accuracy = evaluate_acc(y = labels_test, y_hat = y_pred)\n",
    "print(f'test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "Currently processing instance 0\n",
      "Currently processing instance 1000\n",
      "Currently processing instance 2000\n",
      "Currently processing instance 3000\n",
      "Currently processing instance 4000\n",
      "Currently processing instance 5000\n",
      "Currently processing instance 6000\n",
      "Currently processing instance 7000\n",
      "Currently processing instance 8000\n",
      "Currently processing instance 9000\n",
      "Currently processing instance 10000\n",
      "Currently processing instance 11000\n",
      "Currently processing instance 12000\n",
      "Currently processing instance 13000\n",
      "Currently processing instance 14000\n",
      "Currently processing instance 15000\n",
      "Currently processing instance 16000\n",
      "Currently processing instance 17000\n",
      "Currently processing instance 18000\n",
      "Currently processing instance 19000\n",
      "Currently processing instance 20000\n",
      "Currently processing instance 21000\n",
      "Currently processing instance 22000\n",
      "Currently processing instance 23000\n",
      "Currently processing instance 24000\n",
      "test accuracy: 0.52144\n"
     ]
    }
   ],
   "source": [
    "model_imdb_v5 = MultinomialNaiveBayes()\n",
    "model_imdb_v5.fit(imdb_train_v5.toarray(), labels_train_imdb)\n",
    "y_prob = model_imdb_v5.predict(imdb_test_v5.toarray())\n",
    "# check_sum =  np.sum(y_prob,axis = 1)\n",
    "# print(y_prob.shape)\n",
    "# print(y_prob)\n",
    "# print(check_sum.T)\n",
    "y_pred = np.argmax(y_prob, axis = 1)\n",
    "# print(y_pred.shape)\n",
    "accuracy = evaluate_acc(y = labels_test_imdb, y_hat = y_pred)\n",
    "print(f'test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54152\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(imdb_train_v5, labels_train_imdb)\n",
    "y_pred_sklearn = clf.predict(imdb_test_v5)\n",
    "accuracy_sklearn = evaluate_acc(y = labels_test_imdb, y_hat = y_pred_sklearn)\n",
    "print(accuracy_sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVdFrBUISsft"
   },
   "source": [
    "# Task 3: Run experiments and comapre with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ2JC3o7mIIK"
   },
   "source": [
    "## Building logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfvlNIRqmGp-",
    "outputId": "d608219f-067c-46f8-a01c-44ad698531b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d3b64abb3b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtwenty_train_LR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabel_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwenty_train_LR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_test_v2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlabel_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "#Testing the accuracy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "twenty_train_LR = LogisticRegression(random_state=0,max_iter=200).fit(twenty_train_v2, labels_train)\n",
    "label_predict = twenty_train_LR.predict(twenty_test_v2)\n",
    "label_predict.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use sklearn multinomialNB() to veriify that the model is implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6322357939458311\n"
     ]
    }
   ],
   "source": [
    "clf2 = MultinomialNB()\n",
    "clf2.fit(twenty_train_v2, labels_train)\n",
    "y_pred_sklearn = clf2.predict(twenty_test_v2)\n",
    "accuracy_sklearn = evaluate_acc(y = labels_test, y_hat = y_pred_sklearn)\n",
    "print(accuracy_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COMP 551 - P2 - Task 1 - Data Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
